{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d43355dc",
   "metadata": {},
   "source": [
    "# Data aquisition \n",
    "\n",
    "We used the **PRAW** (Python Reddit API Wrapper) library to programmatically access Reddit data through its API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0e78443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in /Users/greeshma/Desktop/DSW Project/.venv/lib/python3.9/site-packages (7.8.1)\n",
      "Requirement already satisfied: pandas in /Users/greeshma/Desktop/DSW Project/.venv/lib/python3.9/site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy in /Users/greeshma/Desktop/DSW Project/.venv/lib/python3.9/site-packages (2.0.2)\n",
      "Requirement already satisfied: prawcore<3,>=2.4 in /Users/greeshma/Desktop/DSW Project/.venv/lib/python3.9/site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: update_checker>=0.18 in /Users/greeshma/Desktop/DSW Project/.venv/lib/python3.9/site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /Users/greeshma/Desktop/DSW Project/.venv/lib/python3.9/site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in /Users/greeshma/Desktop/DSW Project/.venv/lib/python3.9/site-packages (from prawcore<3,>=2.4->praw) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/greeshma/Desktop/DSW Project/.venv/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/greeshma/Desktop/DSW Project/.venv/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/greeshma/Desktop/DSW Project/.venv/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/greeshma/Desktop/DSW Project/.venv/lib/python3.9/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/greeshma/Desktop/DSW Project/.venv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/greeshma/Desktop/DSW Project/.venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/greeshma/Desktop/DSW Project/.venv/lib/python3.9/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/greeshma/Desktop/DSW Project/.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install praw pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92711c77",
   "metadata": {},
   "source": [
    "Importing the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52fad201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/greeshma/Desktop/DSW Project/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be5f761",
   "metadata": {},
   "source": [
    "Setting up credentials required to authenticate with Reddit's API using PRAW:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "061e865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIG ---\n",
    "CLIENT_ID = '3Ptv1n3uzKL-RaqAQnrMlg'\n",
    "CLIENT_SECRET = 'pa5OheU7NtiIw6jl5MaFAz8ouLrZDQ'\n",
    "USER_AGENT = 'reddit-popularity-predictor'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bce562",
   "metadata": {},
   "source": [
    "Fetching posts from a range of different subbreddits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be5a93e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBREDDITS = ['technology', 'sports', 'funny', 'science', 'politics', 'gaming', 'movies']\n",
    "POSTS_PER_SUBREDDIT = 750\n",
    "SAMPLE_PER_BUCKET = 300 # how many posts per popularity bucket to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d75681d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Reddit API\n",
    "reddit = praw.Reddit(\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    user_agent=USER_AGENT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e537529",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_posts(subreddit, sort, limit):\n",
    "    \"\"\"Fetch posts from a subreddit with given sort and limit.\"\"\"\n",
    "    posts = []\n",
    "    submissions = getattr(reddit.subreddit(subreddit), sort)(limit=limit)\n",
    "    for submission in submissions:\n",
    "        posts.append({\n",
    "            'subreddit': subreddit,\n",
    "            'id': submission.id,\n",
    "            'title': submission.title,\n",
    "            'selftext': submission.selftext,\n",
    "            'score': submission.score,\n",
    "            'num_comments': submission.num_comments,\n",
    "            'created_utc': submission.created_utc,\n",
    "            'flair': submission.link_flair_text,\n",
    "            'upvote_ratio': submission.upvote_ratio,\n",
    "            'is_self': submission.is_self,\n",
    "            'nsfw': submission.over_18,\n",
    "            'author': str(submission.author),\n",
    "            'url': submission.url,\n",
    "            'sort_type': sort\n",
    "        })\n",
    "    return posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e66aa375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching new posts from r/technology...\n",
      "Fetching new posts from r/sports...\n",
      "Fetching new posts from r/funny...\n",
      "Fetching new posts from r/science...\n",
      "Fetching new posts from r/politics...\n",
      "Fetching new posts from r/gaming...\n",
      "Fetching new posts from r/movies...\n",
      "Fetching top posts from r/technology...\n",
      "Fetching top posts from r/sports...\n",
      "Fetching top posts from r/funny...\n",
      "Fetching top posts from r/science...\n",
      "Fetching top posts from r/politics...\n",
      "Fetching top posts from r/gaming...\n",
      "Fetching top posts from r/movies...\n",
      "Total posts before bucketing: 10108\n"
     ]
    }
   ],
   "source": [
    "all_posts = []\n",
    "\n",
    "# Fetch different types of posts (new posts, top posts)\n",
    "for sub in SUBREDDITS:\n",
    "    print(f\"Fetching new posts from r/{sub}...\")\n",
    "    all_posts.extend(fetch_posts(sub, 'new', POSTS_PER_SUBREDDIT))\n",
    "\n",
    "for sub in SUBREDDITS:\n",
    "    print(f\"Fetching top posts from r/{sub}...\")\n",
    "    all_posts.extend(fetch_posts(sub, 'top', POSTS_PER_SUBREDDIT))\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(all_posts)\n",
    "\n",
    "# Remove duplicates (some posts may appear in both new and top)\n",
    "df = df.drop_duplicates(subset='id')\n",
    "\n",
    "print(f\"Total posts before bucketing: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72868d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>flair</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>is_self</th>\n",
       "      <th>nsfw</th>\n",
       "      <th>author</th>\n",
       "      <th>url</th>\n",
       "      <th>sort_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>technology</td>\n",
       "      <td>1lzgoop</td>\n",
       "      <td>Disaster Looms As President Trump Plans To Def...</td>\n",
       "      <td></td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>1.752480e+09</td>\n",
       "      <td>Space</td>\n",
       "      <td>0.93</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>upyoars</td>\n",
       "      <td>https://autos.yahoo.com/articles/disaster-loom...</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>technology</td>\n",
       "      <td>1lzfv56</td>\n",
       "      <td>You can still enable uBlock Origin in Chrome, ...</td>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>1.752477e+09</td>\n",
       "      <td>Software</td>\n",
       "      <td>0.58</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>moeka_8962</td>\n",
       "      <td>https://www.neowin.net/guides/you-can-still-en...</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>technology</td>\n",
       "      <td>1lzfoze</td>\n",
       "      <td>Japan using generative AI less than other coun...</td>\n",
       "      <td></td>\n",
       "      <td>409</td>\n",
       "      <td>43</td>\n",
       "      <td>1.752477e+09</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>0.96</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>moeka_8962</td>\n",
       "      <td>https://www3.nhk.or.jp/nhkworld/en/news/202507...</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>technology</td>\n",
       "      <td>1lze324</td>\n",
       "      <td>‘Fossil fuel flunkies’: US senator warns of Bi...</td>\n",
       "      <td></td>\n",
       "      <td>100</td>\n",
       "      <td>3</td>\n",
       "      <td>1.752471e+09</td>\n",
       "      <td>Energy</td>\n",
       "      <td>0.96</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>upyoars</td>\n",
       "      <td>https://www.straitstimes.com/world/united-stat...</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>technology</td>\n",
       "      <td>1lzdxu7</td>\n",
       "      <td>Security vulnerability on U.S. trains that let...</td>\n",
       "      <td></td>\n",
       "      <td>152</td>\n",
       "      <td>17</td>\n",
       "      <td>1.752470e+09</td>\n",
       "      <td>Security</td>\n",
       "      <td>0.97</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SelflessMirror</td>\n",
       "      <td>https://www.tomshardware.com/tech-industry/cyb...</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subreddit       id                                              title  \\\n",
       "0  technology  1lzgoop  Disaster Looms As President Trump Plans To Def...   \n",
       "1  technology  1lzfv56  You can still enable uBlock Origin in Chrome, ...   \n",
       "2  technology  1lzfoze  Japan using generative AI less than other coun...   \n",
       "3  technology  1lze324  ‘Fossil fuel flunkies’: US senator warns of Bi...   \n",
       "4  technology  1lzdxu7  Security vulnerability on U.S. trains that let...   \n",
       "\n",
       "  selftext  score  num_comments   created_utc                    flair  \\\n",
       "0              80             6  1.752480e+09                    Space   \n",
       "1               6            16  1.752477e+09                 Software   \n",
       "2             409            43  1.752477e+09  Artificial Intelligence   \n",
       "3             100             3  1.752471e+09                   Energy   \n",
       "4             152            17  1.752470e+09                 Security   \n",
       "\n",
       "   upvote_ratio  is_self   nsfw          author  \\\n",
       "0          0.93    False  False         upyoars   \n",
       "1          0.58    False  False      moeka_8962   \n",
       "2          0.96    False  False      moeka_8962   \n",
       "3          0.96    False  False         upyoars   \n",
       "4          0.97    False  False  SelflessMirror   \n",
       "\n",
       "                                                 url sort_type  \n",
       "0  https://autos.yahoo.com/articles/disaster-loom...       new  \n",
       "1  https://www.neowin.net/guides/you-can-still-en...       new  \n",
       "2  https://www3.nhk.or.jp/nhkworld/en/news/202507...       new  \n",
       "3  https://www.straitstimes.com/world/united-stat...       new  \n",
       "4  https://www.tomshardware.com/tech-industry/cyb...       new  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349f83fe",
   "metadata": {},
   "source": [
    "Instead of predicting Reddit post scores (a regression task), we simplify the problem into a classificatiion task by categorizing the scores into buckets (low, medium, high popularity)\n",
    "\n",
    "The post scores are divided into the three categories based on quantiles. This helps to transform the continuous `score` into a new categorical variable, `popularity_bucket` which can be useful for classification models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "848d43b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "popularity_bucket\n",
      "high      3437\n",
      "low       3336\n",
      "medium    3335\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Bucket scores into low/medium/high popularity ---\n",
    "\n",
    "# Define buckets by score quantiles or fixed thresholds\n",
    "# Here: Use quantiles to split into 3 equal groups\n",
    "\n",
    "quantiles = df['score'].quantile([0.33, 0.66]).values\n",
    "low_threshold, high_threshold = quantiles[0], quantiles[1]\n",
    "\n",
    "def bucket_score(score):\n",
    "    if score <= low_threshold:\n",
    "        return 'low'\n",
    "    elif score <= high_threshold:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'high'\n",
    "\n",
    "df['popularity_bucket'] = df['score'].apply(bucket_score)\n",
    "\n",
    "print(df['popularity_bucket'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ba8bd0",
   "metadata": {},
   "source": [
    "Since the dataset is already balanced across the `popularity_bucket` categories, we don#t need to apply additional sampling techniques to balance the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fda28b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset: 10108\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of the dataset:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd70b9f0",
   "metadata": {},
   "source": [
    "Finally, we save the data to a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcff0c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset to reddit_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Save dataset ---\n",
    "df.to_csv('../data/reddit_dataset.csv', index=False)\n",
    "print(\"Saved dataset to reddit_dataset.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
