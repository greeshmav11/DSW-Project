{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d43355dc",
   "metadata": {},
   "source": [
    "# Data aquisition \n",
    "\n",
    "We used the **PRAW** (Python Reddit API Wrapper) library to programmatically access Reddit data through its API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e78443",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install praw pandas numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92711c77",
   "metadata": {},
   "source": [
    "Importing the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52fad201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be5f761",
   "metadata": {},
   "source": [
    "Setting up credentials required to authenticate with Reddit's API using PRAW:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061e865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIG ---\n",
    "CLIENT_ID = '3Ptv1n3uzKL-RaqAQnrMlg'\n",
    "CLIENT_SECRET = 'pa5OheU7NtiIw6jl5MaFAz8ouLrZDQ'\n",
    "USER_AGENT = 'reddit-popularity-predictor'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bce562",
   "metadata": {},
   "source": [
    "Fetching posts from a range of different subbreddits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5a93e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBREDDITS = ['technology', 'sports', 'funny', 'science', 'politics', 'gaming', 'movies']\n",
    "POSTS_PER_SUBREDDIT = 750\n",
    "SAMPLE_PER_BUCKET = 300 # how many posts per popularity bucket to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d75681d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Reddit API\n",
    "reddit = praw.Reddit(\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    user_agent=USER_AGENT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e537529",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_posts(subreddit, sort, limit):\n",
    "    \"\"\"Fetch posts from a subreddit with given sort and limit.\"\"\"\n",
    "    posts = []\n",
    "    submissions = getattr(reddit.subreddit(subreddit), sort)(limit=limit)\n",
    "    for submission in submissions:\n",
    "        posts.append({\n",
    "            'subreddit': subreddit,\n",
    "            'id': submission.id,\n",
    "            'title': submission.title,\n",
    "            'selftext': submission.selftext,\n",
    "            'score': submission.score,\n",
    "            'num_comments': submission.num_comments,\n",
    "            'created_utc': submission.created_utc,\n",
    "            'flair': submission.link_flair_text,\n",
    "            'upvote_ratio': submission.upvote_ratio,\n",
    "            'is_self': submission.is_self,\n",
    "            'nsfw': submission.over_18,\n",
    "            'author': str(submission.author),\n",
    "            'url': submission.url,\n",
    "            'sort_type': sort\n",
    "        })\n",
    "    return posts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66aa375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching new posts from r/technology...\n",
      "Fetching new posts from r/sports...\n",
      "Fetching new posts from r/funny...\n",
      "Fetching new posts from r/science...\n",
      "Fetching new posts from r/politics...\n",
      "Fetching new posts from r/gaming...\n",
      "Fetching new posts from r/movies...\n",
      "Fetching top posts from r/technology...\n",
      "Fetching top posts from r/sports...\n",
      "Fetching top posts from r/funny...\n",
      "Fetching top posts from r/science...\n",
      "Fetching top posts from r/politics...\n",
      "Fetching top posts from r/gaming...\n",
      "Fetching top posts from r/movies...\n",
      "Total posts before bucketing: 10095\n"
     ]
    }
   ],
   "source": [
    "all_posts = []\n",
    "\n",
    "# Fetch different types of posts (new posts, top posts)\n",
    "for sub in SUBREDDITS:\n",
    "    print(f\"Fetching new posts from r/{sub}...\")\n",
    "    all_posts.extend(fetch_posts(sub, 'new', POSTS_PER_SUBREDDIT))\n",
    "\n",
    "for sub in SUBREDDITS:\n",
    "    print(f\"Fetching top posts from r/{sub}...\")\n",
    "    all_posts.extend(fetch_posts(sub, 'top', POSTS_PER_SUBREDDIT))\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(all_posts)\n",
    "\n",
    "# Remove duplicates (some posts may appear in both new and top)\n",
    "df = df.drop_duplicates(subset='id')\n",
    "\n",
    "print(f\"Total posts before bucketing: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72868d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>flair</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>is_self</th>\n",
       "      <th>nsfw</th>\n",
       "      <th>author</th>\n",
       "      <th>url</th>\n",
       "      <th>sort_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>technology</td>\n",
       "      <td>1ly9hhz</td>\n",
       "      <td>Generative AI is Turning Publishing Into a Swa...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.752350e+09</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>0.50</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>ubcstaffer123</td>\n",
       "      <td>https://www.pastemagazine.com/books/publishing...</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>technology</td>\n",
       "      <td>1ly9dpp</td>\n",
       "      <td>Can AI help prevent homelessness?</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.752350e+09</td>\n",
       "      <td>Society</td>\n",
       "      <td>0.40</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>ubcstaffer123</td>\n",
       "      <td>https://www.thedesertreview.com/news/state/can...</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>technology</td>\n",
       "      <td>1ly851k</td>\n",
       "      <td>Why forecasters are concerned about losing 3 k...</td>\n",
       "      <td></td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>1.752347e+09</td>\n",
       "      <td>Society</td>\n",
       "      <td>0.94</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Wagamaga</td>\n",
       "      <td>https://www.pbs.org/newshour/science/why-forec...</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>technology</td>\n",
       "      <td>1ly7o4a</td>\n",
       "      <td>Is AI Taking Over Influencers' Jobs?</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>1.752345e+09</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>0.33</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>ThickTrack1492</td>\n",
       "      <td>https://www.aiviralclub.com/is-ai-taking-over-...</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>technology</td>\n",
       "      <td>1ly6fe5</td>\n",
       "      <td>The Trump Administration Is Planning to Use AI...</td>\n",
       "      <td></td>\n",
       "      <td>3369</td>\n",
       "      <td>237</td>\n",
       "      <td>1.752342e+09</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>0.98</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Aggravating_Money992</td>\n",
       "      <td>https://truthout.org/articles/the-trump-admini...</td>\n",
       "      <td>new</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subreddit       id                                              title  \\\n",
       "0  technology  1ly9hhz  Generative AI is Turning Publishing Into a Swa...   \n",
       "1  technology  1ly9dpp                  Can AI help prevent homelessness?   \n",
       "2  technology  1ly851k  Why forecasters are concerned about losing 3 k...   \n",
       "3  technology  1ly7o4a               Is AI Taking Over Influencers' Jobs?   \n",
       "4  technology  1ly6fe5  The Trump Administration Is Planning to Use AI...   \n",
       "\n",
       "  selftext  score  num_comments   created_utc                    flair  \\\n",
       "0               0             0  1.752350e+09  Artificial Intelligence   \n",
       "1               0             5  1.752350e+09                  Society   \n",
       "2              30             6  1.752347e+09                  Society   \n",
       "3               0            22  1.752345e+09  Artificial Intelligence   \n",
       "4            3369           237  1.752342e+09  Artificial Intelligence   \n",
       "\n",
       "   upvote_ratio  is_self   nsfw                author  \\\n",
       "0          0.50    False  False         ubcstaffer123   \n",
       "1          0.40    False  False         ubcstaffer123   \n",
       "2          0.94    False  False              Wagamaga   \n",
       "3          0.33    False  False        ThickTrack1492   \n",
       "4          0.98    False  False  Aggravating_Money992   \n",
       "\n",
       "                                                 url sort_type  \n",
       "0  https://www.pastemagazine.com/books/publishing...       new  \n",
       "1  https://www.thedesertreview.com/news/state/can...       new  \n",
       "2  https://www.pbs.org/newshour/science/why-forec...       new  \n",
       "3  https://www.aiviralclub.com/is-ai-taking-over-...       new  \n",
       "4  https://truthout.org/articles/the-trump-admini...       new  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349f83fe",
   "metadata": {},
   "source": [
    "Instead of predicting Reddit post scores (a regression task), we simplify the problem into a classificatiion task by categorizing the scores into buckets (low, medium, high popularity)\n",
    "\n",
    "The post scores are divided into the three categories based on quantiles. This helps to transform the continuous `score` into a new categorical variable, `popularity_bucket` which can be useful for classification models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "848d43b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "popularity_bucket\n",
      "high      3432\n",
      "low       3333\n",
      "medium    3330\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Bucket scores into low/medium/high popularity ---\n",
    "\n",
    "# Define buckets by score quantiles or fixed thresholds\n",
    "# Here: Use quantiles to split into 3 equal groups\n",
    "\n",
    "quantiles = df['score'].quantile([0.33, 0.66]).values\n",
    "low_threshold, high_threshold = quantiles[0], quantiles[1]\n",
    "\n",
    "def bucket_score(score):\n",
    "    if score <= low_threshold:\n",
    "        return 'low'\n",
    "    elif score <= high_threshold:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'high'\n",
    "\n",
    "df['popularity_bucket'] = df['score'].apply(bucket_score)\n",
    "\n",
    "print(df['popularity_bucket'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ba8bd0",
   "metadata": {},
   "source": [
    "Since the dataset is already balanced across the `popularity_bucket` categories, we don#t need to apply additional sampling techniques to balance the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fda28b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset: 10095\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of the dataset:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd70b9f0",
   "metadata": {},
   "source": [
    "Finally, we save the data to a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bcff0c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset to reddit_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Save dataset ---\n",
    "df.to_csv('reddit_dataset.csv', index=False)\n",
    "print(\"Saved dataset to reddit_dataset.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
