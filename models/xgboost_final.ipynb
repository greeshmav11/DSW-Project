{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4a4a777",
   "metadata": {},
   "source": [
    "# Model 1 : XGBoost\n",
    "\n",
    "\n",
    " This notebook performs classification of Reddit posts into popularity buckets using XGBoost, leveraging TF-IDF features extracted from post titles along with other structured categorical features. The workflow includes data preprocessing, baseline evaluation, model training,cross-validation, hyperparameter tuning with Optuna, and model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6017b928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix \n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.metrics import (\n",
    "    cohen_kappa_score,\n",
    "    matthews_corrcoef,\n",
    "    log_loss \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb313ba2",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "08c410a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>flair</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>is_self</th>\n",
       "      <th>nsfw</th>\n",
       "      <th>author</th>\n",
       "      <th>sort_type</th>\n",
       "      <th>popularity_bucket</th>\n",
       "      <th>created_hour</th>\n",
       "      <th>media_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>technology</td>\n",
       "      <td>1lvds7w</td>\n",
       "      <td>students can’t use ai to cheat on standardized...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>ubcstaffer123</td>\n",
       "      <td>new</td>\n",
       "      <td>low</td>\n",
       "      <td>8</td>\n",
       "      <td>external_link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>technology</td>\n",
       "      <td>1lvdi5e</td>\n",
       "      <td>instagram wrongly accuses some users of breach...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>Social Media</td>\n",
       "      <td>1.00</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>zsreport</td>\n",
       "      <td>new</td>\n",
       "      <td>low</td>\n",
       "      <td>8</td>\n",
       "      <td>external_link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>technology</td>\n",
       "      <td>1lvcxoa</td>\n",
       "      <td>turkey blocks x's grok chatbot for alleged ins...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>Social Media</td>\n",
       "      <td>0.91</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>BreakfastTop6899</td>\n",
       "      <td>new</td>\n",
       "      <td>low</td>\n",
       "      <td>7</td>\n",
       "      <td>external_link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>technology</td>\n",
       "      <td>1lvai0d</td>\n",
       "      <td>globalfoundries to make risc-v cpus — fab acqu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>Hardware</td>\n",
       "      <td>0.83</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>jhansonxi</td>\n",
       "      <td>new</td>\n",
       "      <td>low</td>\n",
       "      <td>5</td>\n",
       "      <td>external_link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>technology</td>\n",
       "      <td>1lv9syt</td>\n",
       "      <td>rubio impersonation campaign underscores broad...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>Artificial Intelligence</td>\n",
       "      <td>0.82</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>BreakfastTop6899</td>\n",
       "      <td>new</td>\n",
       "      <td>low</td>\n",
       "      <td>4</td>\n",
       "      <td>external_link</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subreddit       id                                              title  \\\n",
       "0  technology  1lvds7w  students can’t use ai to cheat on standardized...   \n",
       "1  technology  1lvdi5e  instagram wrongly accuses some users of breach...   \n",
       "2  technology  1lvcxoa  turkey blocks x's grok chatbot for alleged ins...   \n",
       "3  technology  1lvai0d  globalfoundries to make risc-v cpus — fab acqu...   \n",
       "4  technology  1lv9syt  rubio impersonation campaign underscores broad...   \n",
       "\n",
       "  selftext  score  num_comments                    flair  upvote_ratio  \\\n",
       "0      NaN      2             2  Artificial Intelligence          1.00   \n",
       "1      NaN      9             2             Social Media          1.00   \n",
       "2      NaN     18             3             Social Media          0.91   \n",
       "3      NaN     18             1                 Hardware          0.83   \n",
       "4      NaN     25             3  Artificial Intelligence          0.82   \n",
       "\n",
       "   is_self   nsfw            author sort_type popularity_bucket  created_hour  \\\n",
       "0    False  False     ubcstaffer123       new               low             8   \n",
       "1    False  False          zsreport       new               low             8   \n",
       "2    False  False  BreakfastTop6899       new               low             7   \n",
       "3    False  False         jhansonxi       new               low             5   \n",
       "4    False  False  BreakfastTop6899       new               low             4   \n",
       "\n",
       "      media_type  \n",
       "0  external_link  \n",
       "1  external_link  \n",
       "2  external_link  \n",
       "3  external_link  \n",
       "4  external_link  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "\n",
    "df = pd.read_csv(\"../data/cleaned_reddit_posts.csv\")\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f707c26",
   "metadata": {},
   "source": [
    "We first drop unnecessary columns, such as id and score (we drop score as it has been converted to popularity_bucket field). \n",
    "We also drop the columns upvote_ratio and num_comments, as these fields are highly correlated to the output and could potentially leak the output label. \n",
    "Author field is dropped as it has very high cardinality (5032 unique values out of 10047 rows), and would not aid in prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0071eb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are not used for prediction\n",
    "\n",
    "drop_cols = ['score', 'upvote_ratio', 'sort_type', 'id', 'author', 'selftext','num_comments']\n",
    "df = df.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd208606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TF-IDF vectorization on the 'title' column\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "X_title_tfidf = tfidf.fit_transform(df['title']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6430dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical features using Label Encoding\n",
    "\n",
    "label_enc_cols = ['subreddit', 'flair', 'media_type']\n",
    "for col in label_enc_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf4f64d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target variable 'popularity_bucket'\n",
    "\n",
    "target_le = LabelEncoder()\n",
    "df['popularity_bucket'] = target_le.fit_transform(df['popularity_bucket'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9d23192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert TF-IDF output to DataFrame for easy concatenation\n",
    "\n",
    "tfidf_df = pd.DataFrame(\n",
    "    X_title_tfidf,\n",
    "    columns=[f'tfidf_{i}' for i in range(X_title_tfidf.shape[1])],\n",
    "    index=df.index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7c400469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare structured feature DataFrame by removing target and raw text\n",
    "\n",
    "structured_df = df.drop(columns=['popularity_bucket', 'title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b564354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into features and labels\n",
    "# Concatenate TF-IDF features with structured features to form final feature matrix X\n",
    "\n",
    "X = pd.concat([tfidf_df, structured_df], axis=1)\n",
    "y = df['popularity_bucket']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efe483a",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "34fb4952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training + testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,                # input and output\n",
    "    test_size=0.2,       # 20% for testing, 80% for training\n",
    "    random_state=42,\n",
    "    stratify=y           # ensures same label distribution in train & test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457bd237",
   "metadata": {},
   "source": [
    "### Baseline evaluation using majority class prediction\n",
    "\n",
    " We create a simple baseline model that always predicts the most common class from the training data. This helps us understand how well a real model should perform by comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7719fdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "popularity_bucket\n",
      "0    3415\n",
      "1    3316\n",
      "2    3316\n",
      "Name: count, dtype: int64\n",
      "Majority class in training data: 0\n",
      "\n",
      "Baseline Performance (Majority Class):\n",
      "Accuracy: 0.3398\n",
      "F1 Score (macro): 0.1691\n",
      "Cohen's Kappa: 0.0\n",
      "Matthews Correlation Coefficient (MCC): 0.0\n"
     ]
    }
   ],
   "source": [
    "# Baseline: Naive majority class \n",
    "\n",
    "\n",
    "print(df[\"popularity_bucket\"].value_counts())\n",
    "\n",
    "# Identify majority class in training data\n",
    "majority_class = Counter(y_train).most_common(1)[0][0]\n",
    "print(\"Majority class in training data:\", majority_class)\n",
    "\n",
    "# Predict majority class for all test samples as a naive baseline\n",
    "y_pred_baseline = np.full_like(y_test, fill_value=majority_class)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"\\nBaseline Performance (Majority Class):\")\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred_baseline), 4))\n",
    "print(\"F1 Score (macro):\", round(f1_score(y_test, y_pred_baseline, average='macro'), 4))\n",
    "print(\"Cohen's Kappa:\", round(cohen_kappa_score(y_test, y_pred_baseline), 4))\n",
    "print(\"Matthews Correlation Coefficient (MCC):\", round(matthews_corrcoef(y_test, y_pred_baseline), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3cff0a",
   "metadata": {},
   "source": [
    "### Train Initial XGBoost Model\n",
    "\n",
    "We train a basic XGBoost classifier on the training data and evaluate its performance on the test set. This gives us a strong starting point before doing any tuning or cross-validation.\n",
    "\n",
    "Initialize the Model\n",
    "- We create an instance of XGBClassifier with the following key settings:\n",
    "\n",
    "    - `use_label_encoder`=`False`: Avoids warnings in new XGBoost versions.\n",
    "\n",
    "    - `eval_metric`=`'mlogloss'`: Specifies the evaluation metric suitable for multiclass classification.\n",
    "\n",
    "    - `tree_method`=`'hist'` and `device`=`'cuda'`: Enable fast training using GPU and histogram-based tree building.\n",
    "\n",
    "    - `random_state`=`42`: Ensures reproducibility.\n",
    "\n",
    "Train the Model\n",
    "\n",
    "- The model is trained on the full training set `X_train` and `y_train`.\n",
    "\n",
    "Predict on the Test Set\n",
    "- We use `predict()` on `X_test` to get predictions for unseen data.\n",
    "\n",
    "Evaluate the Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "18c1f146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:27:08] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:27:08] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:27:08] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=&#x27;cuda&#x27;, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=&#x27;cuda&#x27;, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device='cuda', early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='mlogloss',\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train an XGBOOST Model\n",
    "\n",
    "# Initialize the model\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', tree_method=\"hist\", device=\"cuda\", random_state=42)\n",
    "\n",
    "# Train the model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6e5f626a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline XGBoost Performance:\n",
      "Accuracy: 0.6587064676616915\n",
      "F1 Score (macro): 0.6589554675730152\n",
      "Cohen's Kappa: 0.4878083027311988\n",
      "Matthews Correlation Coefficient (MCC): 0.4889894433662448\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.72      0.69       683\n",
      "           1       0.74      0.64      0.68       664\n",
      "           2       0.59      0.62      0.60       663\n",
      "\n",
      "    accuracy                           0.66      2010\n",
      "   macro avg       0.66      0.66      0.66      2010\n",
      "weighted avg       0.66      0.66      0.66      2010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test set\n",
    "\n",
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Baseline XGBoost Performance:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1 Score (macro):\", f1_score(y_test, y_pred, average='macro'))\n",
    "print(\"Cohen's Kappa:\", cohen_kappa_score(y_test, y_pred))\n",
    "print(\"Matthews Correlation Coefficient (MCC):\", matthews_corrcoef(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5861d167",
   "metadata": {},
   "source": [
    "###  Simple Validation Split\n",
    "\n",
    " To check how well our XGBoost model performs before full training, we do a basic train-validation split.\n",
    "- First, we split the original training data again: 80% for training and 20% for validation. This helps us evaluate the model on unseen data during the training phase itself (not the test set).\n",
    "- We train a new XGBoost model on this smaller training set (`X_train_final`, `y_train_final`).\n",
    "- Then we make predictions on the validation set (`X_val`) and calculate performance metrics:\n",
    "    - Accuracy – how many predictions were correct\n",
    "    - F1 Score (macro) – balances precision and recall across all classes\n",
    "    - Cohen's Kappa – measures agreement between predicted and true labels\n",
    "    - Matthews Correlation Coefficient (MCC) – a balanced metric even for imbalanced data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "56261874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;mlogloss&#x27;,\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='mlogloss',\n",
       "              feature_types=None, feature_weights=None, gamma=None,\n",
       "              grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple CV\n",
    "\n",
    "# Step 1: Split into train (80%) and test (20%) - already done\n",
    "\n",
    "# Now: Split train into train (80%) and val (20%)\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_train  \n",
    ")\n",
    "\n",
    "# Step 2: Train on training set\n",
    "xgb_model_simple_cv = xgb.XGBClassifier(objective='multi:softprob', eval_metric='mlogloss')\n",
    "xgb_model_simple_cv.fit(X_train_final, y_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "240a10b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.650497512437811\n",
      "Validation F1 Score (macro): 0.6521165917867692\n",
      "Validation Cohen's Kappa: 0.47564923290804473\n",
      "Validation Matthews Correlation Coefficient (MCC): 0.47647637337113347\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Evaluate on validation set\n",
    "y_val_pred = xgb_model_simple_cv.predict(X_val)\n",
    "\n",
    "\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_val_pred))\n",
    "print(\"Validation F1 Score (macro):\", f1_score(y_val, y_val_pred, average='macro'))\n",
    "print(\"Validation Cohen's Kappa:\", cohen_kappa_score(y_val, y_val_pred))\n",
    "print(\"Validation Matthews Correlation Coefficient (MCC):\", matthews_corrcoef(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa18a7da",
   "metadata": {},
   "source": [
    "### 5-Fold Cross Validation\n",
    "\n",
    "To get a better estimate of how well our XGBoost model will perform on new, unseen data, we use 5-fold cross-validation.\n",
    "\n",
    "How it works?\n",
    "\n",
    "- We divide the original training set (`X_train`, `y_train`) into 5 equal parts (folds) using `StratifiedKFold`.\n",
    "\n",
    "- For each fold (fold 1 to fold 5):\n",
    "\n",
    "    - Four parts are used for training → `X_tr`, `y_tr`\n",
    "\n",
    "    - The remaining one part is used for validation → `X_val_fold`, `y_val_fold`\n",
    "\n",
    "- This process ensures that:\n",
    "\n",
    "    - Every sample is used once for validation. The class distribution remains balanced in all folds.\n",
    "\n",
    "- Inside each fold:\n",
    "\n",
    "    - The model is trained on `X_tr`, `y_tr`.\n",
    "\n",
    "    - Predictions are made on `X_val_fold`.\n",
    "\n",
    "    - We compute the following metrics:\n",
    "\n",
    "        - `acc`: Accuracy\n",
    "\n",
    "        - `f1`: F1 Score (macro)\n",
    "\n",
    "        - `kappa`: Cohen's Kappa\n",
    "\n",
    "        - `mcc`: Matthews Correlation Coefficient\n",
    "    \n",
    "- Final step:\n",
    "\n",
    "    - After all 5 folds, we calculate the average and standard deviation of these metrics:\n",
    "\n",
    "        - np.mean(accuracies) ± np.std(accuracies)\n",
    "\n",
    "        - np.mean(f1_scores) ± np.std(f1_scores) and so on for Kappa and MCC\n",
    "\n",
    "This gives us a robust estimate of model performance across different subsets of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ff507360",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:27:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:27:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:27:39] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Acc: 0.6604 | F1: 0.6612 | Kappa: 0.4905 | MCC: 0.4912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:27:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:27:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:27:45] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - Acc: 0.6549 | F1: 0.6562 | Kappa: 0.4822 | MCC: 0.4833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:27:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:27:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:27:55] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - Acc: 0.6397 | F1: 0.6397 | Kappa: 0.4592 | MCC: 0.4608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:28:03] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:28:03] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:28:03] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 - Acc: 0.6534 | F1: 0.6525 | Kappa: 0.4797 | MCC: 0.4809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:28:12] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:28:12] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:28:12] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - Acc: 0.6403 | F1: 0.6427 | Kappa: 0.4604 | MCC: 0.4635\n",
      "\n",
      "K-Fold CV Results (5 folds):\n",
      "Avg Accuracy: 0.6497 ± 0.0083\n",
      "Avg F1 Score: 0.6505 ± 0.0081\n",
      "Avg Cohen's Kappa:0.4744 ± 0.0124\n",
      "Avg MCC:          0.4760 ± 0.0118\n"
     ]
    }
   ],
   "source": [
    "# 5-Fold CV\n",
    "\n",
    "# Set up Stratified K-Fold\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store metrics for each fold\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "kappas = []\n",
    "mccs = []\n",
    "\n",
    "# Loop through each fold\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train, y_train), 1):\n",
    "\n",
    "    # Split data into train and validation based on fold\n",
    "    X_tr, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val_fold = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "    # Initialize model\n",
    "    model = xgb.XGBClassifier(objective='multi:softprob', eval_metric='mlogloss', use_label_encoder=False, tree_method=\"hist\", device=\"cuda\", random_state=42)\n",
    "\n",
    "    # Train on training fold\n",
    "    model.fit(X_tr, y_tr)\n",
    "\n",
    "    # Predict on validation fold\n",
    "    y_pred_fold = model.predict(X_val_fold)\n",
    "\n",
    "    # Evaluate metrics\n",
    "    acc = accuracy_score(y_val_fold, y_pred_fold)\n",
    "    f1 = f1_score(y_val_fold, y_pred_fold, average='macro')\n",
    "    kappa = cohen_kappa_score(y_val_fold, y_pred_fold)\n",
    "    mcc = matthews_corrcoef(y_val_fold, y_pred_fold)\n",
    "\n",
    "    accuracies.append(acc)\n",
    "    f1_scores.append(f1)\n",
    "    kappas.append(kappa)\n",
    "    mccs.append(mcc)\n",
    "\n",
    "    print(f\"Fold {fold} - Acc: {acc:.4f} | F1: {f1:.4f} | Kappa: {kappa:.4f} | MCC: {mcc:.4f}\")\n",
    "\n",
    "# After all folds\n",
    "print(\"\\nK-Fold CV Results (5 folds):\")\n",
    "print(f\"Avg Accuracy: {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\")\n",
    "print(f\"Avg F1 Score: {np.mean(f1_scores):.4f} ± {np.std(f1_scores):.4f}\")\n",
    "print(f\"Avg Cohen's Kappa:{np.mean(kappas):.4f} ± {np.std(kappas):.4f}\")\n",
    "print(f\"Avg MCC:          {np.mean(mccs):.4f} ± {np.std(mccs):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a75c0e0",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning with Optuna\n",
    "\n",
    "We use Optuna, a powerful automatic hyperparameter optimization framework, to find the best combination of hyperparameters for our XGBoost model.\n",
    "\n",
    "Objective Function:\n",
    "\n",
    "- We define an `objective()` function which:\n",
    "\n",
    "    - Suggests hyperparameters for each trial (like `n_estimators`, `max_depth`, `learning_rate`, etc.)\n",
    "\n",
    "    - Trains an XGBoost model using `X_train`, `y_train`\n",
    "\n",
    "    - Predicts on the test set `X_test`\n",
    "\n",
    "    - Returns the macro F1 score, which Optuna tries to maximize.\n",
    "\n",
    "- Main hyperparameters being tuned:\n",
    "\n",
    "    - `n_estimators`: Number of boosting rounds (trees)\n",
    "\n",
    "    - `max_depth`: Maximum depth of each tree\n",
    "\n",
    "    - `learning_rate`: Controls how quickly the model learns\n",
    "\n",
    "    - `subsample` & `colsample_bytree`: Control how much of the data/features are used per tree\n",
    "\n",
    "    - `gamma`, `reg_alpha`, `reg_lambda`: Regularization terms to reduce overfitting\n",
    "\n",
    "\n",
    "\n",
    "- We create an Optuna study that:\n",
    "\n",
    "    - Tries multiple hyperparameter combinations (here, `n_trials`=30)\n",
    "\n",
    "    - Uses TPE (Tree-structured Parzen Estimator) for sampling\n",
    "\n",
    "    - Uses Median Pruner to stop underperforming trials early\n",
    "\n",
    "\n",
    "- After running 30 trials:\n",
    "\n",
    "    - The best F1 score and corresponding parameters are printed. These can be used to train the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b2c606",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-16 11:34:19,332] A new study created in memory with name: no-name-7ed7dade-a325-44a6-b457-70c74519d20e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:34:19] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:34:19] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-07-16 11:34:46,963] Trial 0 finished with value: 0.6278435121902273 and parameters: {'n_estimators': 409, 'max_depth': 3, 'learning_rate': 0.27487516950247404, 'subsample': 0.8301942550488062, 'colsample_bytree': 0.6378727933854071, 'gamma': 4.892959493619313, 'reg_lambda': 0.7064235425679269, 'reg_alpha': 9.404745993286204}. Best is trial 0 with value: 0.6278435121902273.\n",
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:34:47] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:34:47] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-07-16 11:35:21,866] Trial 1 finished with value: 0.660646366307356 and parameters: {'n_estimators': 178, 'max_depth': 8, 'learning_rate': 0.1692451892774248, 'subsample': 0.568376754170316, 'colsample_bytree': 0.9397740680621963, 'gamma': 0.3882452991196633, 'reg_lambda': 7.683629377175145, 'reg_alpha': 3.0094593778390046}. Best is trial 1 with value: 0.660646366307356.\n",
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:35:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:35:22] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-07-16 11:36:00,215] Trial 2 finished with value: 0.6556431667427741 and parameters: {'n_estimators': 435, 'max_depth': 6, 'learning_rate': 0.24146528632253664, 'subsample': 0.6675138011777318, 'colsample_bytree': 0.9115022885226365, 'gamma': 1.9649782082051543, 'reg_lambda': 4.596164437578741, 'reg_alpha': 2.8906634358700507}. Best is trial 1 with value: 0.660646366307356.\n",
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:36:00] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:36:00] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-07-16 11:36:25,970] Trial 3 finished with value: 0.6616560310827827 and parameters: {'n_estimators': 283, 'max_depth': 8, 'learning_rate': 0.2110999223359582, 'subsample': 0.7071505395696376, 'colsample_bytree': 0.7859617505621581, 'gamma': 4.251667751218394, 'reg_lambda': 0.9370836298331021, 'reg_alpha': 0.47084711608536645}. Best is trial 3 with value: 0.6616560310827827.\n",
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:36:26] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:36:26] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-07-16 11:37:22,828] Trial 4 finished with value: 0.6318485323958464 and parameters: {'n_estimators': 497, 'max_depth': 4, 'learning_rate': 0.0258093565984117, 'subsample': 0.773295565832899, 'colsample_bytree': 0.9466985925686706, 'gamma': 2.9460200417438154, 'reg_lambda': 4.838374446044268, 'reg_alpha': 8.21658348442488}. Best is trial 3 with value: 0.6616560310827827.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  F1 Score: 0.6616560310827827\n",
      "  Best hyperparameters:\n",
      "    n_estimators: 283\n",
      "    max_depth: 8\n",
      "    learning_rate: 0.2110999223359582\n",
      "    subsample: 0.7071505395696376\n",
      "    colsample_bytree: 0.7859617505621581\n",
      "    gamma: 4.251667751218394\n",
      "    reg_lambda: 0.9370836298331021\n",
      "    reg_alpha: 0.47084711608536645\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning using optuna\n",
    "\n",
    "# Objective function that Optuna will optimize\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0),\n",
    "        'random_state': 42,\n",
    "        'eval_metric': 'mlogloss',\n",
    "        'tree_method':'hist', \n",
    "        'device':'cuda'\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, preds, average='macro')\n",
    "    return f1                                           # Optuna will try to maximize this\n",
    "\n",
    "# Create the study and run it\n",
    "study = optuna.create_study(direction='maximize', sampler=TPESampler(), pruner=MedianPruner())\n",
    "study.optimize(objective, n_trials=40)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best trial:\")\n",
    "print(f\"  F1 Score: {study.best_value}\")\n",
    "print(\"  Best hyperparameters:\")\n",
    "for key, val in study.best_params.items():\n",
    "    print(f\"    {key}: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f5efc8",
   "metadata": {},
   "source": [
    "### Retrain Model with Best Hyperparameters\n",
    "\n",
    "We use the best hyperparameters found by Optuna to train our final XGBoost model and evaluate its performance.\n",
    "\n",
    "- Update and Prepare Best Parameters:\n",
    "    - We get the best parameter set from `study.best_trial.params`.\n",
    "\n",
    "    - Then, we add required parameters to this set:\n",
    "\n",
    "        - `objective`: Defines the task — here, multi:softprob for multiclass classification.\n",
    "\n",
    "        - `num_class`: Tells the model how many classes (popularity buckets) we have.\n",
    "\n",
    "        - `use_label_encoder`: Disabled to avoid warnings.\n",
    "\n",
    "        - `eval_metric`: We use mlogloss, a common metric for multiclass problems.\n",
    "\n",
    "        - `tree_method` and `device`: Speed up training using histogram-based trees and GPU.\n",
    "\n",
    "- Train Final Model\n",
    "\n",
    "We train the XGBoost model (`final_model`) using all of the training data (`X_train`, `y_train`) and the best settings from tuning.\n",
    "\n",
    "`predict()` gives the final class predictions on the test set.\n",
    "\n",
    "`predict_proba()` gives the predicted probabilities for each class — needed for computing log loss.\n",
    "\n",
    "- We then evaluate the model on:\n",
    "\n",
    "    - Accuracy: Overall correct predictions\n",
    "\n",
    "    - F1 Score (macro): F1 score for each class, averaged equally\n",
    "\n",
    "    - Cohen’s Kappa: Measures agreement between predictions and true labels\n",
    "\n",
    "    - Matthews Correlation Coefficient (MCC): A robust metric even for imbalanced classes\n",
    "\n",
    "    - Log Loss: Penalizes incorrect predictions with high confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "55ddad51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:45:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:49: No visible GPU is found, setting device to CPU.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:45:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "c:\\Users\\dsouz\\DSW-Project\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [11:45:33] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Evaluation After Tuning:\n",
      "Accuracy: 0.6686567164179105\n",
      "F1 Score (macro): 0.6688951832706106\n",
      "Cohen's Kappa: 0.5027306611907332\n",
      "Matthews Correlation Coefficient (MCC): 0.504629853530596\n",
      "Log Loss: 0.7875904708191223\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.73      0.70       683\n",
      "           1       0.76      0.62      0.68       664\n",
      "           2       0.60      0.65      0.62       663\n",
      "\n",
      "    accuracy                           0.67      2010\n",
      "   macro avg       0.68      0.67      0.67      2010\n",
      "weighted avg       0.68      0.67      0.67      2010\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrain the model using the best parameters\n",
    "best_params = study.best_trial.params\n",
    "best_params.update({\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": len(target_le.classes_),\n",
    "    \"use_label_encoder\": False,\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"device\": \"cuda\"\n",
    "})\n",
    "\n",
    "final_model = xgb.XGBClassifier(**best_params)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_best = final_model.predict(X_test)\n",
    "y_proba = final_model.predict_proba(X_test)\n",
    "\n",
    "# Final test performance\n",
    "print(\"Final Evaluation After Tuning:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_pred_best))\n",
    "print(\"F1 Score (macro):\", f1_score(y_test, y_test_pred_best, average='macro'))\n",
    "print(\"Cohen's Kappa:\", cohen_kappa_score(y_test, y_test_pred_best))\n",
    "print(\"Matthews Correlation Coefficient (MCC):\", matthews_corrcoef(y_test, y_test_pred_best))\n",
    "print(\"Log Loss:\", log_loss(y_test, y_proba))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1bb1c787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix plot\n",
    "cm = confusion_matrix(y_test, y_test_pred_best)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_le.classes_, yticklabels=target_le.classes_)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(\"confusion_matrix.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dc382fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance plot\n",
    "importances = final_model.feature_importances_\n",
    "top_idx = importances.argsort()[-20:][::-1]   # Top 20 important features\n",
    "top_features = X.columns[top_idx]\n",
    "\n",
    "sns.barplot(x=importances[top_idx], y=top_features)\n",
    "plt.title(\"Top 20 Feature Importances\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"feature_importance.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "032c824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Explainer and Plots\n",
    "\n",
    "# Create SHAP explainer\n",
    "explainer = shap.Explainer(final_model)\n",
    "\n",
    "# Compute SHAP values on test set\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Summary plot: global feature importance\n",
    "shap.summary_plot(shap_values, X_test, feature_names=X.columns, show=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"shap_summary_plot.png\")\n",
    "plt.close()\n",
    "\n",
    "# Dependence plot for the top feature from your feature importance\n",
    "top_feature = top_features[0]\n",
    "class_idx = 0  # Choose the class index you want to visualize (0, 1, or 2 for your 3 classes)\n",
    "shap.dependence_plot(top_feature, shap_values.values[:, :, class_idx], X_test, feature_names=X.columns, show=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"shap_dependence_{top_feature}.png\")\n",
    "plt.close()\n",
    "\n",
    "\n",
    "final_model.save_model(\"xgb_model.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
