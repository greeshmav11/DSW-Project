{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b3a866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7891d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_wsj_news_filtered(start_date, end_date, delay=1.5):\n",
    "    # Define stock-market related keywords\n",
    "    keywords = [\n",
    "        \"stock\", \"stocks\", \"market\", \"S&P\", \"Nasdaq\", \"Dow\", \"shares\",\n",
    "        \"investors\", \"bond\", \"inflation\", \"recession\", \"rate hike\",\n",
    "        \"earnings\", \"profit\", \"loss\", \"fed\", \"Federal Reserve\", \"interest rate\",\n",
    "        \"volatility\", \"rally\", \"crash\", \"selloff\", \"bull market\", \"bear market\",\n",
    "        \"guidance\", \"forecast\", \"jobs report\", \"layoffs\", \"unemployment\", \"bank\", \"treasury\"\n",
    "    ]\n",
    "\n",
    "    current_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    stop_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    all_articles = []\n",
    "\n",
    "    while current_date <= stop_date:\n",
    "        url = f\"https://www.wsj.com/news/archive/{current_date.strftime('%Y/%m/%d')}\"\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"[{current_date.strftime('%Y-%m-%d')}] Failed: {response.status_code}\")\n",
    "            current_date += timedelta(days=1)\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        articles = soup.find_all(\"article\")\n",
    "        count_kept = 0\n",
    "\n",
    "        for article in articles:\n",
    "            h2 = article.find(\"h2\")\n",
    "            if h2:\n",
    "                headline = h2.get_text(strip=True)\n",
    "                headline_lower = headline.lower()\n",
    "                if any(kw in headline_lower for kw in keywords):\n",
    "                    link = h2.find(\"a\")[\"href\"] if h2.find(\"a\") else \"\"\n",
    "                    all_articles.append({\n",
    "                        \"date\": current_date.strftime(\"%Y-%m-%d\"),\n",
    "                        \"headline\": headline,\n",
    "                        \"url\": link\n",
    "                    })\n",
    "                    count_kept += 1\n",
    "\n",
    "        print(f\"[{current_date.strftime('%Y-%m-%d')}] {count_kept} relevant headlines kept\")\n",
    "        time.sleep(delay)\n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    return pd.DataFrame(all_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bad2822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_sp500_labels(start_date, end_date):\n",
    "    import yfinance as yf\n",
    "    import pandas as pd\n",
    "\n",
    "    # Download S&P 500 data\n",
    "    df = yf.download(\"^GSPC\", start=start_date, end=end_date, auto_adjust=True)\n",
    "\n",
    "    # Use 'Close' or fallback to 'Adj Close'\n",
    "    if \"Close\" in df.columns:\n",
    "        close_series = df[\"Close\"]\n",
    "    elif \"Adj Close\" in df.columns:\n",
    "        close_series = df[\"Adj Close\"]\n",
    "    else:\n",
    "        raise KeyError(\"Neither 'Close' nor 'Adj Close' found in data\")\n",
    "\n",
    "    # Shift the close series\n",
    "    next_close = close_series.shift(-1)\n",
    "\n",
    "    # Create DataFrame â€” ensure everything is 1D\n",
    "    data = pd.DataFrame({\n",
    "        \"Date\": close_series.index.to_list(),\n",
    "        \"Close\": close_series.to_numpy().flatten(),\n",
    "        \"Next_Close\": next_close.to_numpy().flatten()\n",
    "    })\n",
    "\n",
    "    # Generate binary labels\n",
    "    data[\"Label\"] = (data[\"Next_Close\"] > data[\"Close\"]).astype(int)\n",
    "\n",
    "    # Drop last row (no next close available)\n",
    "    data.dropna(inplace=True)\n",
    "\n",
    "    # Format date\n",
    "    data[\"date\"] = pd.to_datetime(data[\"Date\"]).dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    return data[[\"date\", \"Label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e644105b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_news_and_labels(news_df, stock_df):\n",
    "    merged = pd.merge(news_df, stock_df, on=\"date\")\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff18ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Pick a small date range for testing\n",
    "start_date = \"2023-06-01\"\n",
    "end_date = \"2023-06-30\"\n",
    "\n",
    "# Run the WSJ scraper\n",
    "news_df = scrape_wsj_news_filtered(start_date, end_date)\n",
    "\n",
    "# Save the result\n",
    "news_df.to_csv(\"data/wsj_news.csv\", index=False)\n",
    "print(f\"\\n Saved {len(news_df)} filtered headlines to data/wsj_news.csv\")\n",
    "\n",
    "\n",
    "# Get S&P 500 labels\n",
    "stock_df = download_sp500_labels(start_date, end_date)\n",
    "stock_df.to_csv(\"data/sp500_labels.csv\", index=False)\n",
    "print(f\" Saved {len(stock_df)} rows of stock movement labels to data/sp500_labels.csv\")\n",
    "\n",
    "    \n",
    "# Merge news with stock labels\n",
    "merged_df = merge_news_and_labels(news_df, stock_df)\n",
    "merged_df.to_csv(\"data/merged_news_market.csv\", index=False)\n",
    "print(f\" Merged data saved to data/merged_news_market.csv with {len(merged_df)} rows\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
